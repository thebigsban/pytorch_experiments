{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "59ad4a6f-70ae-4ebc-b0fd-e4c5ec8588f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "import einops\n",
    "from typing import Dict, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "070e4f32-9ada-4db8-8e7d-563a7af46e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):                                  # new class with parent class nn.Module\n",
    "    \"\"\"\n",
    "    Multi-Headed Attention\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,                                                       # d, embedding dim\n",
    "        num_heads,                                                       # number of heads\n",
    "        kdim = None,                                                     # d except in first layer or query != key\n",
    "        vdim = None,                                                     # d except in first layer (i think?)\n",
    "        dropout=0.0,                                                     # dropout\n",
    "        bias = False,                                                     # whether to add bias or not\n",
    "#        add_bias_kv: bool = False,                                       #\n",
    "#        add_zero_attn: bool = False,                                     # \n",
    "        self_attention: bool = False,                                    #\n",
    "#        encoder_decoder_attention: bool = False,                         # \n",
    "#        use_rotary_embeddings: bool = False,                             #\n",
    "    ):\n",
    "        super().__init__()                                               # necessary to have MHA be able to call functions from nn.Module\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim              \n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self.qkv_same_dim = self.kdim == embed_dim and \\\n",
    "            self.vdim == embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout                                           # dropout, randomly select a few nodes to drop during training\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (                                                         # head_dim = embed_dim/num_heads\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.self_attention = self_attention\n",
    "#        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        assert not self.self_attention or self.qkv_same_dim, (           \n",
    "            \"Self-attention requires query, key \"+\\\n",
    "            \"and \" \"value to be of the same size\"\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(self.kdim, embed_dim, bias=bias)          # Q,K,V projection\n",
    "        self.v_proj = nn.Linear(self.vdim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)        # linear layer after attention \n",
    "        self.reset_parameters()                                           # initialize weights\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask = None,\n",
    "    ):\n",
    "        seqlen, batchsize, embed_dim = query.size()                        # input (seqlen, batchsize, embed_dim)\n",
    "        assert embed_dim == self.embed_dim \n",
    "        \n",
    "        q = einops.rearrange(self.q_proj(query), 's b (h d)->b h s d'\\\n",
    "                             , h=self.num_heads)                           # project qkv, reshape to have batchsize * numheads for mha\n",
    "        k = einops.rearrange(self.k_proj(key), 's b (h d)->b h s d'\\\n",
    "                             , h=self.num_heads)                           # transpose because F.scaled_dot_product_attm expects (..., seqlen, embed_dim)\n",
    "        v = einops.rearrange(self.v_proj(value), 's b (h d)->b h s d'\\\n",
    "                             , h=self.num_heads)                           # but since we have multiple heads, \n",
    "                                                                           # embed_dim for each head is d/num_heads\n",
    "\n",
    "        q *= self.scaling\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask)\n",
    "        attn = einops.rearrange(attn, 'b h s d->s b (h d)', h = self.num_heads)\n",
    "        output = self.out_proj(attn)\n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.qkv_same_dim:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        # if self.out_proj.bias is not None:\n",
    "        #     nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "        # if self.bias_k is not None:\n",
    "        #     nn.init.xavier_normal_(self.bias_k)\n",
    "        # if self.bias_v is not None:\n",
    "        #     nn.init.xavier_normal_(self.bias_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b137fe03-7e86-40df-883a-8cd2bd1d10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MultiHeadedAttention(embed_dim=10, num_heads=1,self_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "22eb00c8-e7d6-4e3e-88f6-19571a3b3c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3738, -0.1501,  0.1794, -0.3177, -0.3394, -0.3143,  0.0425,  0.2653,\n",
       "          0.2233, -0.3119],\n",
       "        [ 0.0346,  0.2435, -0.2388,  0.2066, -0.2687, -0.0800, -0.1075,  0.2745,\n",
       "         -0.1055, -0.1391],\n",
       "        [-0.2789,  0.2672, -0.1322,  0.0066, -0.0223,  0.2218,  0.1279,  0.3076,\n",
       "          0.0131, -0.1986],\n",
       "        [-0.3606, -0.0517,  0.1176, -0.3512, -0.0435,  0.3113, -0.3524,  0.0343,\n",
       "         -0.0336, -0.1623],\n",
       "        [ 0.0200, -0.0513,  0.1541,  0.3259,  0.1133, -0.0918, -0.0164, -0.2219,\n",
       "         -0.2189,  0.2120],\n",
       "        [-0.2219,  0.2433,  0.1217, -0.1107, -0.0285,  0.3464, -0.0170, -0.2445,\n",
       "         -0.2170,  0.0898],\n",
       "        [-0.1868, -0.1109, -0.3041,  0.2479, -0.2599,  0.0785, -0.2531,  0.1866,\n",
       "          0.3535, -0.2657],\n",
       "        [-0.3117,  0.1409, -0.1129,  0.3302,  0.0410,  0.0379, -0.3021, -0.1481,\n",
       "         -0.1434, -0.0277],\n",
       "        [ 0.1617,  0.2814, -0.1642,  0.2836,  0.2458,  0.2910,  0.1934,  0.3113,\n",
       "         -0.0816, -0.2578],\n",
       "        [-0.0840,  0.3291, -0.3276,  0.0978,  0.1521,  0.2480,  0.1403,  0.0688,\n",
       "         -0.3486,  0.3321]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.k_proj.state_dict()['weight']\n",
    "#print(test.k_proj.state_dict()['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3bad4c2-5356-4015-ac2c-b19300e5adce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros((10,1,10))\n",
    "input[0,0,0] = 1\n",
    "input[3,0,0] = 1\n",
    "input[6,0,5] = 1\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "524d5498-090d-4722-8935-d579b6dc5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test.forward(input, input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ffbe35fc-981e-4d5c-b771-d6706bdc98ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0608, -0.0304, -0.0043,  0.0067,  0.0411,  0.1107,  0.0527,\n",
      "           0.0396, -0.0316,  0.0286]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0608, -0.0304, -0.0043,  0.0067,  0.0411,  0.1107,  0.0527,\n",
      "           0.0396, -0.0316,  0.0286]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0600, -0.0296, -0.0055,  0.0075,  0.0418,  0.1104,  0.0522,\n",
      "           0.0390, -0.0308,  0.0278]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]],\n",
      "\n",
      "        [[ 0.0602, -0.0298, -0.0052,  0.0073,  0.0416,  0.1104,  0.0523,\n",
      "           0.0391, -0.0310,  0.0280]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "38ae453a-0971-4cd1-810f-76ffdbc78ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "932c0530-6102-4805-a5c1-f45bcb96fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention_ESM2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,                                                       # d, embedding dim\n",
    "        num_heads,                                                       # number of heads\n",
    "        kdim = None,                                                     # d except in first layer or query != key\n",
    "        vdim = None,                                                     # d except in first layer (i think?)\n",
    "        dropout=0.0,                                                     # dropout\n",
    "        bias = False,                                                    # whether to add bias or not\n",
    "        self_attention: bool = False,                                        #\n",
    "    \n",
    "#        add_bias_kv: bool = False,                                       #\n",
    "#        add_zero_attn: bool = False,                                     # \n",
    "#        encoder_decoder_attention: bool = False,                         # \n",
    "#        use_rotary_embeddings: bool = False,                             #\n",
    "    ):\n",
    "        super().__init__()                                               # necessary to have MHA be able to call functions from nn.Module\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim              \n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self.qkv_same_dim = self.kdim == embed_dim and \\\n",
    "            self.vdim == embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout                                           # dropout, randomly select a few nodes to drop during training\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (                                                         # head_dim = embed_dim/num_heads\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.self_attention = self_attention\n",
    "        #self.encoder_decoder_attention = encoder_decoder_attention      # not going to use this ever for myself i think\n",
    "        assert not self.self_attention or self.qkv_same_dim, (           \n",
    "            \"Self-attention requires query, key \"+\\\n",
    "            \"and \" \"value to be of the same size\"\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(self.kdim, embed_dim, bias=bias)          # Q,K,V projection\n",
    "        self.v_proj = nn.Linear(self.vdim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)        # linear layer after attention \n",
    "        \n",
    "        if add_bias_kv:                                                   # whether to add biases to KV matrices\n",
    "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "        \n",
    "        self.add_zero_attn = add_zero_attn\n",
    "        self.reset_parameters()                                           # initialize weights\n",
    "        self.onnx_trace = False\n",
    "        self.rot_emb = None\n",
    "        if use_rotary_embeddings:\n",
    "            self.rot_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "\n",
    "        self.enable_torch_version = False\n",
    "        if hasattr(F, \"multi_head_attention_forward\"):\n",
    "            self.enable_torch_version = True\n",
    "        else:\n",
    "            self.enable_torch_version = False\n",
    "    def reset_parameters(self):\n",
    "        if self.qkv_same_dim:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key: Optional[Tensor],\n",
    "        value: Optional[Tensor],\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        #need_weights: bool = True,\n",
    "        static_kv: bool = False,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        before_softmax: bool = False,\n",
    "        #need_head_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input shape: (seqlen, batch, encode_dim)\n",
    "        key_padding_mask: (batch, seqlen) where padding elements are 1\n",
    "        attn_mask: usually for causal attention, maybe we use this for packing multiple seqs\n",
    "        \"\"\"\n",
    "        #if need_head_weights:\n",
    "        #    need_weights = True\n",
    "\n",
    "        seqlen, batchsz, embed_dim = query.size()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a9034-482f-4851-be8d-43207a323ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
